{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8165e6ff",
   "metadata": {},
   "source": [
    "# Experiment 1: Random 70-30 Split\n",
    "\n",
    "Baseline experiment with stratified random split:\n",
    "- 70% of labeled data for training\n",
    "- 30% of labeled data for testing\n",
    "- Stratified split maintains class distribution\n",
    "- Uses class weights to handle imbalance (9.8% illicit vs 90.2% licit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc08763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "classes_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n",
    "edgelist_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n",
    "features_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dstr5ch1h",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nganjihin/Desktop/NUS/BT4012/Final Proj/elliptic/bt4012_elliptic/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "g74cyqri607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 203768\n",
      "Number of features: 166\n",
      "Number of edges: 234353\n",
      "Label distribution: Unknown=157204, Illicit=4545, Licit=42019\n"
     ]
    }
   ],
   "source": [
    "# Prepare node features\n",
    "features = features_df.iloc[:, 1:].values\n",
    "node_ids = features_df.iloc[:, 0].values\n",
    "\n",
    "# Create node ID to index mapping\n",
    "node_id_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "\n",
    "# Prepare labels - CORRECT MAPPING: '1'=illicit, '2'=licit\n",
    "# Map to: 0=unknown, 1=illicit, 2=licit\n",
    "classes_df['class'] = classes_df['class'].map({'unknown': 0, '1': 1, '2': 2})\n",
    "labels = np.zeros(len(node_ids), dtype=int)\n",
    "for _, row in classes_df.iterrows():\n",
    "    if row['txId'] in node_id_to_idx:\n",
    "        labels[node_id_to_idx[row['txId']]] = row['class']\n",
    "\n",
    "# Prepare edge index\n",
    "edge_list = []\n",
    "for _, row in edgelist_df.iterrows():\n",
    "    if row['txId1'] in node_id_to_idx and row['txId2'] in node_id_to_idx:\n",
    "        edge_list.append([node_id_to_idx[row['txId1']], node_id_to_idx[row['txId2']]])\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(f\"Number of nodes: {x.shape[0]}\")\n",
    "print(f\"Number of features: {x.shape[1]}\")\n",
    "print(f\"Number of edges: {edge_index.shape[1]}\")\n",
    "print(f\"Label distribution: Unknown={sum(y==0)}, Illicit={sum(y==1)}, Licit={sum(y==2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "htsyjlidomt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training nodes: 32594\n",
      "Test nodes: 13970\n",
      "Unknown nodes: 157204\n"
     ]
    }
   ],
   "source": [
    "# Create train/test split (only on labeled nodes)\n",
    "labeled_mask = y > 0\n",
    "labeled_indices = torch.where(labeled_mask)[0].numpy()\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    labeled_indices, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y[labeled_indices].numpy()\n",
    ")\n",
    "\n",
    "# Create masks\n",
    "train_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "print(f\"Training nodes: {train_mask.sum()}\")\n",
    "print(f\"Test nodes: {test_mask.sum()}\")\n",
    "print(f\"Unknown nodes: {(~labeled_mask).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaw4l85x6th",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNFraudDetector(\n",
      "  (gcn1): GCNConv(166, 64)\n",
      "  (gcn2): GCNConv(64, 32)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GCNFraudDetector(torch.nn.Module):\n",
    "    def __init__(self, in_features=166, hidden=64, out_classes=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(in_features, hidden)\n",
    "        self.gcn2 = GCNConv(hidden, hidden // 2)\n",
    "        self.fc = torch.nn.Linear(hidden // 2, out_classes)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # GCN Layer 1\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # GCN Layer 2\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = GCNFraudDetector(in_features=x.shape[1], hidden=64, out_classes=3, dropout=0.5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ei64meng9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Class distribution in training set:\n",
      "  Class 1 (Illicit):  3181 samples (9.8%)\n",
      "  Class 2 (Licit):    29413 samples (90.2%)\n",
      "\n",
      "Class weights:\n",
      "  Class 0 (Unknown): 1.0000 (not used in training)\n",
      "  Class 1 (Illicit):  5.1232\n",
      "  Class 2 (Licit):    0.5541\n",
      "\n",
      "Weight ratio (Illicit/Licit): 9.25x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = x.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "y = y.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "test_mask = test_mask.to(device)\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "# Only consider labeled classes (1=illicit, 2=licit) since class 0 (unknown) is not in training\n",
    "train_labels = y[train_mask]\n",
    "n_illicit = (train_labels == 1).sum().float()\n",
    "n_licit = (train_labels == 2).sum().float()\n",
    "total = train_labels.shape[0]\n",
    "\n",
    "# Compute weights: inversely proportional to class frequency\n",
    "# We need weights for all 3 classes (0, 1, 2) even though 0 is not in training\n",
    "weight_illicit = total / (2.0 * n_illicit)  # Higher weight for minority class\n",
    "weight_licit = total / (2.0 * n_licit)      # Lower weight for majority class\n",
    "class_weights = torch.tensor([1.0, weight_illicit, weight_licit], dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"  Class 1 (Illicit):  {n_illicit.int()} samples ({n_illicit/total*100:.1f}%)\")\n",
    "print(f\"  Class 2 (Licit):    {n_licit.int()} samples ({n_licit/total*100:.1f}%)\")\n",
    "print(f\"\\nClass weights:\")\n",
    "print(f\"  Class 0 (Unknown): {class_weights[0]:.4f} (not used in training)\")\n",
    "print(f\"  Class 1 (Illicit):  {class_weights[1]:.4f}\")\n",
    "print(f\"  Class 2 (Licit):    {class_weights[2]:.4f}\")\n",
    "print(f\"\\nWeight ratio (Illicit/Licit): {class_weights[1]/class_weights[2]:.2f}x\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sbyokj19bx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010, Loss: 0.5294, Train Acc: 0.7912, Test Acc: 0.7908\n",
      "Epoch 020, Loss: 0.4571, Train Acc: 0.8130, Test Acc: 0.8118\n",
      "Epoch 030, Loss: 0.4049, Train Acc: 0.8286, Test Acc: 0.8279\n",
      "Epoch 040, Loss: 0.3769, Train Acc: 0.8617, Test Acc: 0.8598\n",
      "Epoch 050, Loss: 0.3508, Train Acc: 0.8707, Test Acc: 0.8684\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, edge_index)\n",
    "    loss = criterion(out[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    out = model(x, edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc, pred[mask].cpu(), out[mask].cpu()\n",
    "\n",
    "# Training loop\n",
    "print(\"Training GCN...\")\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        train_acc, _, _ = test(train_mask)\n",
    "        test_acc, _, _ = test(test_mask)\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pyzccecndg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL EVALUATION METRICS\n",
      "==================================================\n",
      "Accuracy:  86.84%\n",
      "Precision: 41.58%\n",
      "Recall:    85.78%\n",
      "F1 Score:  56.01%\n",
      "AUC-ROC:   0.9447\n",
      "PR-AUC:    0.7329\n",
      "RMSE:      0.3627\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "_, test_pred, test_out = test(test_mask)\n",
    "test_labels = y[test_mask].cpu().numpy()\n",
    "\n",
    "# Convert to binary classification (0=licit, 1=illicit)\n",
    "# Model outputs: 0=unknown, 1=illicit, 2=licit\n",
    "binary_pred = (test_pred.numpy() == 1).astype(int)  # 1 if illicit, 0 otherwise\n",
    "binary_labels = (test_labels == 1).astype(int)  # 1 if illicit, 0 otherwise\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(binary_labels, binary_pred)\n",
    "precision = precision_score(binary_labels, binary_pred, zero_division=0)\n",
    "recall = recall_score(binary_labels, binary_pred, zero_division=0)\n",
    "f1 = f1_score(binary_labels, binary_pred, zero_division=0)\n",
    "\n",
    "# Get probabilities for AUC-ROC and PR-AUC (probability of illicit class)\n",
    "test_probs = F.softmax(test_out, dim=1)[:, 1].numpy()  # Column 1 = illicit\n",
    "auc_roc = roc_auc_score(binary_labels, test_probs)\n",
    "pr_auc = average_precision_score(binary_labels, test_probs)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(binary_labels, binary_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall:    {recall*100:.2f}%\")\n",
    "print(f\"F1 Score:  {f1*100:.2f}%\")\n",
    "print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "print(f\"RMSE:      {rmse:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2jxp9wb",
   "metadata": {},
   "source": [
    "# Experiment 2: Temporal 80-20 Split\n",
    "\n",
    "Instead of random split, we now split based on time steps (column 2 in features).\n",
    "- Time steps range from 1 to 49 (about 2-week intervals)\n",
    "- Train on first 80% of time steps (1-39)\n",
    "- Test on last 20% of time steps (40-49)\n",
    "- This simulates real-world scenario: train on past data, predict future transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "npjq9eam9ls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step analysis:\n",
      "  Time step range: 1 to 49\n",
      "  Unique time steps: 49\n",
      "  Time step distribution:\n",
      "    Time step  1:  7879 nodes (2147 labeled)\n",
      "    Time step  2:  4544 nodes (1117 labeled)\n",
      "    Time step  3:  6621 nodes (1279 labeled)\n",
      "    Time step  4:  5693 nodes (1440 labeled)\n",
      "    Time step  5:  6803 nodes (1882 labeled)\n",
      "    Time step  6:  4328 nodes ( 485 labeled)\n",
      "    Time step  7:  6048 nodes (1203 labeled)\n",
      "    Time step  8:  4457 nodes (1165 labeled)\n",
      "    Time step  9:  4996 nodes ( 778 labeled)\n",
      "    Time step 10:  6727 nodes ( 972 labeled)\n",
      "    Time step 11:  4296 nodes ( 696 labeled)\n",
      "    Time step 12:  2047 nodes ( 506 labeled)\n",
      "    Time step 13:  4528 nodes ( 809 labeled)\n",
      "    Time step 14:  2022 nodes ( 417 labeled)\n",
      "    Time step 15:  3639 nodes ( 618 labeled)\n",
      "    Time step 16:  2975 nodes ( 530 labeled)\n",
      "    Time step 17:  3385 nodes ( 811 labeled)\n",
      "    Time step 18:  1976 nodes ( 389 labeled)\n",
      "    Time step 19:  3506 nodes ( 745 labeled)\n",
      "    Time step 20:  4291 nodes ( 900 labeled)\n",
      "    Time step 21:  3537 nodes ( 641 labeled)\n",
      "    Time step 22:  5894 nodes (1763 labeled)\n",
      "    Time step 23:  4165 nodes (1187 labeled)\n",
      "    Time step 24:  4592 nodes (1126 labeled)\n",
      "    Time step 25:  2314 nodes ( 594 labeled)\n",
      "    Time step 26:  2523 nodes ( 517 labeled)\n",
      "    Time step 27:  1089 nodes ( 206 labeled)\n",
      "    Time step 28:  1653 nodes ( 284 labeled)\n",
      "    Time step 29:  4275 nodes (1174 labeled)\n",
      "    Time step 30:  2483 nodes ( 524 labeled)\n",
      "    Time step 31:  2816 nodes ( 710 labeled)\n",
      "    Time step 32:  4525 nodes (1323 labeled)\n",
      "    Time step 33:  3151 nodes ( 441 labeled)\n",
      "    Time step 34:  2486 nodes ( 515 labeled)\n",
      "    Time step 35:  5507 nodes (1341 labeled)\n",
      "    Time step 36:  6393 nodes (1708 labeled)\n",
      "    Time step 37:  3306 nodes ( 498 labeled)\n",
      "    Time step 38:  2891 nodes ( 756 labeled)\n",
      "    Time step 39:  2760 nodes (1183 labeled)\n",
      "    Time step 40:  4481 nodes (1211 labeled)\n",
      "    Time step 41:  5342 nodes (1132 labeled)\n",
      "    Time step 42:  7140 nodes (2154 labeled)\n",
      "    Time step 43:  5063 nodes (1370 labeled)\n",
      "    Time step 44:  4975 nodes (1591 labeled)\n",
      "    Time step 45:  5598 nodes (1221 labeled)\n",
      "    Time step 46:  3519 nodes ( 712 labeled)\n",
      "    Time step 47:  5121 nodes ( 846 labeled)\n",
      "    Time step 48:  2954 nodes ( 471 labeled)\n",
      "    Time step 49:  2454 nodes ( 476 labeled)\n"
     ]
    }
   ],
   "source": [
    "# Extract time steps (column 1, 0-indexed) from features_df\n",
    "time_steps = features_df.iloc[:, 1].values\n",
    "\n",
    "# Create mapping from node_id to time step\n",
    "time_step_map = {}\n",
    "for idx, node_id in enumerate(node_ids):\n",
    "    time_step_map[node_id] = time_steps[idx]\n",
    "\n",
    "print(\"Time step analysis:\")\n",
    "print(f\"  Time step range: {time_steps.min()} to {time_steps.max()}\")\n",
    "print(f\"  Unique time steps: {len(np.unique(time_steps))}\")\n",
    "print(f\"  Time step distribution:\")\n",
    "for ts in sorted(np.unique(time_steps)):\n",
    "    count = sum(time_steps == ts)\n",
    "    labeled_count = sum((time_steps == ts) & (labels > 0))\n",
    "    print(f\"    Time step {ts:2d}: {count:5d} nodes ({labeled_count:4d} labeled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6s6lzvr5of",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal split at time step 39\n",
      "  Training: time steps 1-39\n",
      "  Testing:  time steps 40-49\n",
      "\n",
      "Temporal split statistics:\n",
      "  Training nodes: 35380\n",
      "  Test nodes:     11184\n",
      "\n",
      "Temporal training set:\n",
      "  Illicit: 3909 (11.0%)\n",
      "  Licit:   31471 (89.0%)\n",
      "\n",
      "Temporal test set:\n",
      "  Illicit: 636 (5.7%)\n",
      "  Licit:   10548 (94.3%)\n"
     ]
    }
   ],
   "source": [
    "# Create temporal train/test split: 80-20 based on time steps\n",
    "# Time steps 1-49, so 80% = first 39 time steps for training, last 10 for testing\n",
    "split_time_step = int(49 * 0.8)  # 39\n",
    "\n",
    "print(f\"Temporal split at time step {split_time_step}\")\n",
    "print(f\"  Training: time steps 1-{split_time_step}\")\n",
    "print(f\"  Testing:  time steps {split_time_step+1}-49\")\n",
    "\n",
    "# Create temporal masks (only for labeled nodes)\n",
    "train_mask_temporal = torch.zeros(len(y), dtype=torch.bool)\n",
    "test_mask_temporal = torch.zeros(len(y), dtype=torch.bool)\n",
    "\n",
    "for idx in range(len(y)):\n",
    "    if y[idx] > 0:  # Only labeled nodes\n",
    "        ts = time_steps[idx]\n",
    "        if ts <= split_time_step:\n",
    "            train_mask_temporal[idx] = True\n",
    "        else:\n",
    "            test_mask_temporal[idx] = True\n",
    "\n",
    "print(f\"\\nTemporal split statistics:\")\n",
    "print(f\"  Training nodes: {train_mask_temporal.sum()}\")\n",
    "print(f\"  Test nodes:     {test_mask_temporal.sum()}\")\n",
    "\n",
    "# Check class distribution in temporal split\n",
    "train_labels_temporal = y[train_mask_temporal]\n",
    "test_labels_temporal = y[test_mask_temporal]\n",
    "\n",
    "print(f\"\\nTemporal training set:\")\n",
    "print(f\"  Illicit: {(train_labels_temporal == 1).sum()} ({(train_labels_temporal == 1).sum()/len(train_labels_temporal)*100:.1f}%)\")\n",
    "print(f\"  Licit:   {(train_labels_temporal == 2).sum()} ({(train_labels_temporal == 2).sum()/len(train_labels_temporal)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTemporal test set:\")\n",
    "print(f\"  Illicit: {(test_labels_temporal == 1).sum()} ({(test_labels_temporal == 1).sum()/len(test_labels_temporal)*100:.1f}%)\")\n",
    "print(f\"  Licit:   {(test_labels_temporal == 2).sum()} ({(test_labels_temporal == 2).sum()/len(test_labels_temporal)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rdvaim0zpgm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal model class weights:\n",
      "  Class 0 (Unknown): 1.0000\n",
      "  Class 1 (Illicit):  4.5255\n",
      "  Class 2 (Licit):    0.5621\n",
      "  Weight ratio (Illicit/Licit): 8.05x\n"
     ]
    }
   ],
   "source": [
    "# Initialize new model for temporal experiment\n",
    "model_temporal = GCNFraudDetector(in_features=x.shape[1], hidden=64, out_classes=3, dropout=0.5)\n",
    "model_temporal = model_temporal.to(device)\n",
    "\n",
    "# Move temporal masks to device\n",
    "train_mask_temporal = train_mask_temporal.to(device)\n",
    "test_mask_temporal = test_mask_temporal.to(device)\n",
    "\n",
    "# Calculate class weights for temporal training set\n",
    "train_labels_temp = y[train_mask_temporal]\n",
    "n_illicit_temp = (train_labels_temp == 1).sum().float()\n",
    "n_licit_temp = (train_labels_temp == 2).sum().float()\n",
    "total_temp = train_labels_temp.shape[0]\n",
    "\n",
    "weight_illicit_temp = total_temp / (2.0 * n_illicit_temp)\n",
    "weight_licit_temp = total_temp / (2.0 * n_licit_temp)\n",
    "class_weights_temporal = torch.tensor([1.0, weight_illicit_temp, weight_licit_temp], dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Temporal model class weights:\")\n",
    "print(f\"  Class 0 (Unknown): {class_weights_temporal[0]:.4f}\")\n",
    "print(f\"  Class 1 (Illicit):  {class_weights_temporal[1]:.4f}\")\n",
    "print(f\"  Class 2 (Licit):    {class_weights_temporal[2]:.4f}\")\n",
    "print(f\"  Weight ratio (Illicit/Licit): {class_weights_temporal[1]/class_weights_temporal[2]:.2f}x\")\n",
    "\n",
    "optimizer_temporal = torch.optim.Adam(model_temporal.parameters(), lr=0.01)\n",
    "criterion_temporal = torch.nn.CrossEntropyLoss(weight=class_weights_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fimkscpntfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN with Temporal Split...\n",
      "Epoch 010, Loss: 0.4921, Train Acc: 0.7680, Test Acc: 0.5290\n",
      "Epoch 020, Loss: 0.4342, Train Acc: 0.8025, Test Acc: 0.6652\n",
      "Epoch 030, Loss: 0.3872, Train Acc: 0.8315, Test Acc: 0.7149\n",
      "Epoch 040, Loss: 0.3559, Train Acc: 0.8601, Test Acc: 0.7946\n",
      "Epoch 050, Loss: 0.3246, Train Acc: 0.8818, Test Acc: 0.8200\n",
      "\n",
      "Temporal training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training functions for temporal model\n",
    "def train_temporal():\n",
    "    model_temporal.train()\n",
    "    optimizer_temporal.zero_grad()\n",
    "    out = model_temporal(x, edge_index)\n",
    "    loss = criterion_temporal(out[train_mask_temporal], y[train_mask_temporal])\n",
    "    loss.backward()\n",
    "    optimizer_temporal.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_temporal(mask):\n",
    "    model_temporal.eval()\n",
    "    out = model_temporal(x, edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc, pred[mask].cpu(), out[mask].cpu()\n",
    "\n",
    "# Training loop for temporal model\n",
    "print(\"Training GCN with Temporal Split...\")\n",
    "for epoch in range(1, 51):\n",
    "    loss = train_temporal()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        train_acc, _, _ = test_temporal(train_mask_temporal)\n",
    "        test_acc, _, _ = test_temporal(test_mask_temporal)\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(\"\\nTemporal training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fmo5ydvyarb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPORAL SPLIT - FINAL EVALUATION METRICS\n",
      "==================================================\n",
      "Accuracy:  82.00%\n",
      "Precision: 18.11%\n",
      "Recall:    61.48%\n",
      "F1 Score:  27.98%\n",
      "AUC-ROC:   0.8181\n",
      "PR-AUC:    0.3507\n",
      "RMSE:      0.4243\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Temporal model evaluation\n",
    "_, test_pred_temporal, test_out_temporal = test_temporal(test_mask_temporal)\n",
    "test_labels_temporal = y[test_mask_temporal].cpu().numpy()\n",
    "\n",
    "# Convert to binary classification\n",
    "binary_pred_temporal = (test_pred_temporal.numpy() == 1).astype(int)\n",
    "binary_labels_temporal = (test_labels_temporal == 1).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_temporal = accuracy_score(binary_labels_temporal, binary_pred_temporal)\n",
    "precision_temporal = precision_score(binary_labels_temporal, binary_pred_temporal, zero_division=0)\n",
    "recall_temporal = recall_score(binary_labels_temporal, binary_pred_temporal, zero_division=0)\n",
    "f1_temporal = f1_score(binary_labels_temporal, binary_pred_temporal, zero_division=0)\n",
    "\n",
    "# Get probabilities\n",
    "test_probs_temporal = F.softmax(test_out_temporal, dim=1)[:, 1].numpy()\n",
    "auc_roc_temporal = roc_auc_score(binary_labels_temporal, test_probs_temporal)\n",
    "pr_auc_temporal = average_precision_score(binary_labels_temporal, test_probs_temporal)\n",
    "\n",
    "# RMSE\n",
    "rmse_temporal = np.sqrt(mean_squared_error(binary_labels_temporal, binary_pred_temporal))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL SPLIT - FINAL EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_temporal*100:.2f}%\")\n",
    "print(f\"Precision: {precision_temporal*100:.2f}%\")\n",
    "print(f\"Recall:    {recall_temporal*100:.2f}%\")\n",
    "print(f\"F1 Score:  {f1_temporal*100:.2f}%\")\n",
    "print(f\"AUC-ROC:   {auc_roc_temporal:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc_temporal:.4f}\")\n",
    "print(f\"RMSE:      {rmse_temporal:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "p53vhxu2gj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEMPORAL SPLIT - DIAGNOSTIC ANALYSIS\n",
      "==================================================\n",
      "\n",
      "1. Test Set Class Distribution:\n",
      "   Illicit: 636\n",
      "   Licit:   10548\n",
      "\n",
      "2. Model Predictions:\n",
      "   Predicted Licit (0):   9025\n",
      "   Predicted Illicit (1): 2159\n",
      "\n",
      "3. Confusion Matrix (Licit=0, Illicit=1):\n",
      "   [[TN FP]\n",
      "    [FN TP]]\n",
      "   [[8780 1768]\n",
      " [ 245  391]]\n",
      "\n",
      "   True Negatives (correctly predicted Licit):   8780\n",
      "   False Positives (Licit predicted as Illicit):  1768\n",
      "   False Negatives (Illicit predicted as Licit):  245\n",
      "   True Positives (correctly predicted Illicit):  391\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Temporal model diagnostic analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL SPLIT - DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1. Test Set Class Distribution:\")\n",
    "print(f\"   Illicit: {sum(binary_labels_temporal == 1)}\")\n",
    "print(f\"   Licit:   {sum(binary_labels_temporal == 0)}\")\n",
    "\n",
    "print(\"\\n2. Model Predictions:\")\n",
    "print(f\"   Predicted Licit (0):   {sum(binary_pred_temporal == 0)}\")\n",
    "print(f\"   Predicted Illicit (1): {sum(binary_pred_temporal == 1)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_temporal = confusion_matrix(binary_labels_temporal, binary_pred_temporal)\n",
    "print(\"\\n3. Confusion Matrix (Licit=0, Illicit=1):\")\n",
    "print(\"   [[TN FP]\")\n",
    "print(\"    [FN TP]]\")\n",
    "print(f\"   {cm_temporal}\")\n",
    "print(f\"\\n   True Negatives (correctly predicted Licit):   {cm_temporal[0,0]}\")\n",
    "print(f\"   False Positives (Licit predicted as Illicit):  {cm_temporal[0,1]}\")\n",
    "print(f\"   False Negatives (Illicit predicted as Licit):  {cm_temporal[1,0]}\")\n",
    "print(f\"   True Positives (correctly predicted Illicit):  {cm_temporal[1,1]}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1a4b2",
   "metadata": {},
   "source": [
    "# Experiment 3: Sequential Fine-tuned GCN\n",
    "\n",
    "Train a single GCN model sequentially through timesteps 1-49:\n",
    "- Initialize model and train on t=1\n",
    "- Fine-tune on each subsequent timestep\n",
    "- Evaluate on each timestep before fine-tuning\n",
    "- This simulates continuous learning as new labeled data arrives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26bb217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential training setup complete\n",
      "Epochs per timestep: 50\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "time_steps_tensor = torch.tensor(time_steps, dtype=torch.long)\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Initialize model\n",
    "model_seq = GCNFraudDetector(in_features=x.shape[1], hidden=64, out_classes=3, dropout=0.5).to(device)\n",
    "optimizer_seq = torch.optim.Adam(model_seq.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def compute_class_weights(labels, device):\n",
    "    n_illicit = (labels == 1).sum().float()\n",
    "    n_licit = (labels == 2).sum().float()\n",
    "    total = len(labels)\n",
    "    if n_illicit == 0 or n_licit == 0:\n",
    "        return torch.tensor([1.0, 1.0, 1.0], dtype=torch.float).to(device)\n",
    "    weight_illicit = total / (2.0 * n_illicit)\n",
    "    weight_licit = total / (2.0 * n_licit)\n",
    "    return torch.tensor([1.0, weight_illicit, weight_licit], dtype=torch.float).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_seq(model, mask):\n",
    "    model.eval()\n",
    "    out = model(x, edge_index)\n",
    "    probs = F.softmax(out, dim=1)\n",
    "    pred = out.argmax(dim=1)\n",
    "    \n",
    "    test_pred = pred[mask].cpu().numpy()\n",
    "    test_labels = y[mask].cpu().numpy()\n",
    "    test_probs = probs[mask][:, 1].cpu().numpy()\n",
    "    \n",
    "    binary_pred = (test_pred == 1).astype(int)\n",
    "    binary_labels = (test_labels == 1).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(binary_labels, binary_pred),\n",
    "        'precision': precision_score(binary_labels, binary_pred, zero_division=0),\n",
    "        'recall': recall_score(binary_labels, binary_pred, zero_division=0),\n",
    "        'f1': f1_score(binary_labels, binary_pred, zero_division=0),\n",
    "        'auc_roc': roc_auc_score(binary_labels, test_probs) if len(np.unique(binary_labels)) > 1 else 0,\n",
    "        'pr_auc': average_precision_score(binary_labels, test_probs) if len(np.unique(binary_labels)) > 1 else 0,\n",
    "    }\n",
    "\n",
    "print(f\"Sequential training setup complete\")\n",
    "print(f\"Epochs per timestep: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "x8lwaav08y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SEQUENTIAL FINE-TUNED GCN TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m         out = model_seq(x, edge_index)\n\u001b[32m     35\u001b[39m         loss = criterion_t(out[train_mask_t], y[train_mask_t])\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m         optimizer_seq.step()\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSequential training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/BT4012/Final Proj/elliptic/bt4012_elliptic/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/BT4012/Final Proj/elliptic/bt4012_elliptic/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/BT4012/Final Proj/elliptic/bt4012_elliptic/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Sequential training loop\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SEQUENTIAL FINE-TUNED GCN TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results_seq = []\n",
    "\n",
    "for t in range(1, 50):\n",
    "    # Create mask for current timestep (labeled nodes only)\n",
    "    train_mask_t = ((time_steps_tensor == t) & (y > 0)).to(device)\n",
    "    n_train = train_mask_t.sum().item()\n",
    "    \n",
    "    if n_train == 0:\n",
    "        print(f\"t={t}: No labeled nodes, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate on current timestep BEFORE training (for t > 1)\n",
    "    if t > 1:\n",
    "        metrics = evaluate_seq(model_seq, train_mask_t)\n",
    "        metrics['timestep'] = t\n",
    "        metrics['n_nodes'] = n_train\n",
    "        all_results_seq.append(metrics)\n",
    "        \n",
    "        if t % 10 == 0:\n",
    "            print(f\"t={t}: F1={metrics['f1']*100:.1f}%, Precision={metrics['precision']*100:.1f}%, Recall={metrics['recall']*100:.1f}%, n={n_train}\")\n",
    "    \n",
    "    # Train/fine-tune on current timestep\n",
    "    class_weights_t = compute_class_weights(y[train_mask_t], device)\n",
    "    criterion_t = torch.nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "    \n",
    "    model_seq.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        optimizer_seq.zero_grad()\n",
    "        out = model_seq(x, edge_index)\n",
    "        loss = criterion_t(out[train_mask_t], y[train_mask_t])\n",
    "        loss.backward()\n",
    "        optimizer_seq.step()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nSequential training completed in {elapsed_time:3.f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hunuuqkfwjs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary\n",
    "results_df_seq = pd.DataFrame(all_results_seq)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SEQUENTIAL FINE-TUNED GCN - RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nAverage metrics across all timesteps (t=2 to t=49):\")\n",
    "print(f\"  Accuracy:  {results_df_seq['accuracy'].mean()*100:.2f}%\")\n",
    "print(f\"  Precision: {results_df_seq['precision'].mean()*100:.2f}%\")\n",
    "print(f\"  Recall:    {results_df_seq['recall'].mean()*100:.2f}%\")\n",
    "print(f\"  F1 Score:  {results_df_seq['f1'].mean()*100:.2f}%\")\n",
    "print(f\"  AUC-ROC:   {results_df_seq['auc_roc'].mean():.4f}\")\n",
    "print(f\"  PR-AUC:    {results_df_seq['pr_auc'].mean():.4f}\")\n",
    "\n",
    "# Metrics for test period (t=40-49) to compare with temporal split\n",
    "test_results = results_df_seq[results_df_seq['timestep'] >= 40]\n",
    "print(f\"\\nMetrics for test period (t=40-49):\")\n",
    "print(f\"  Accuracy:  {test_results['accuracy'].mean()*100:.2f}%\")\n",
    "print(f\"  Precision: {test_results['precision'].mean()*100:.2f}%\")\n",
    "print(f\"  Recall:    {test_results['recall'].mean()*100:.2f}%\")\n",
    "print(f\"  F1 Score:  {test_results['f1'].mean()*100:.2f}%\")\n",
    "print(f\"  AUC-ROC:   {test_results['auc_roc'].mean():.4f}\")\n",
    "print(f\"  PR-AUC:    {test_results['pr_auc'].mean():.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ib48s4ppfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 score over time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# F1 over timesteps\n",
    "axes[0].plot(results_df_seq['timestep'], results_df_seq['f1'], 'b-o', markersize=3)\n",
    "axes[0].axvline(x=39.5, color='r', linestyle='--', label='Train/Test split')\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].set_title('Sequential GCN: F1 Score Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision vs Recall\n",
    "axes[1].plot(results_df_seq['timestep'], results_df_seq['precision'], 'r-o', markersize=3, label='Precision')\n",
    "axes[1].plot(results_df_seq['timestep'], results_df_seq['recall'], 'g-o', markersize=3, label='Recall')\n",
    "axes[1].axvline(x=39.5, color='k', linestyle='--', label='Train/Test split')\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Sequential GCN: Precision vs Recall')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
