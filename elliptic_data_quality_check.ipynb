{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elliptic Bitcoin Dataset - Data Quality Check\n",
    "\n",
    "**Objective:** Assess data quality of the three Elliptic Bitcoin dataset files\n",
    "\n",
    "**Analysis Focus:**\n",
    "1. Missing values\n",
    "2. Duplicate records\n",
    "3. Data type mismatches\n",
    "\n",
    "**Datasets:**\n",
    "- `elliptic_txs_classes.csv` - Transaction labels (illicit/licit/unknown)\n",
    "- `elliptic_txs_edgelist.csv` - Transaction graph edges\n",
    "- `elliptic_txs_features.csv` - Transaction features (166 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset 1: Transaction Classes (`elliptic_txs_classes.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (203769, 2)\n",
      "Columns: ['txId', 'class']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232438397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId    class\n",
       "0  230425980  unknown\n",
       "1    5530458  unknown\n",
       "2  232022460  unknown\n",
       "3  232438397        2\n",
       "4  230460314  unknown"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load transaction classes\n",
    "classes_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n",
    "\n",
    "print(f\"Shape: {classes_df.shape}\")\n",
    "print(f\"Columns: {list(classes_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "classes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      "       Missing_Count  Missing_Percentage\n",
      "txId               0                 0.0\n",
      "class              0                 0.0\n",
      "\n",
      "✓ No missing values detected\n"
     ]
    }
   ],
   "source": [
    "# Count missing values\n",
    "missing_counts = classes_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(classes_df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_summary)\n",
    "\n",
    "if missing_summary['Missing_Count'].sum() == 0:\n",
    "    print(\"\\n✓ No missing values detected\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Total missing values: {missing_summary['Missing_Count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 203,769\n",
      "Unique transaction IDs: 203,769\n",
      "Duplicate transaction IDs: 0\n",
      "\n",
      "✓ No duplicate transaction IDs found\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate transaction IDs\n",
    "n_duplicates = classes_df['txId'].duplicated().sum()\n",
    "n_unique = classes_df['txId'].nunique()\n",
    "n_total = len(classes_df)\n",
    "\n",
    "print(f\"Total records: {n_total:,}\")\n",
    "print(f\"Unique transaction IDs: {n_unique:,}\")\n",
    "print(f\"Duplicate transaction IDs: {n_duplicates:,}\")\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(\"\\n✓ No duplicate transaction IDs found\")\n",
    "else:\n",
    "    print(f\"\\n⚠ {n_duplicates} duplicate transaction IDs detected\")\n",
    "    print(\"\\nDuplicate entries:\")\n",
    "    duplicated_txs = classes_df[classes_df['txId'].duplicated(keep=False)].sort_values('txId')\n",
    "    print(duplicated_txs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "txId      int64\n",
      "class    object\n",
      "dtype: object\n",
      "\n",
      "txId is numeric: True\n",
      "✓ txId has correct data type\n",
      "\n",
      "Class value distribution:\n",
      "class\n",
      "unknown    157205\n",
      "2           42019\n",
      "1            4545\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ All class values are valid ('1', '2', 'unknown')\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"Data Types:\")\n",
    "print(classes_df.dtypes)\n",
    "\n",
    "# Validate txId is numeric\n",
    "is_numeric = pd.api.types.is_numeric_dtype(classes_df['txId'])\n",
    "print(f\"\\ntxId is numeric: {is_numeric}\")\n",
    "\n",
    "if not is_numeric:\n",
    "    print(\"⚠ txId should be numeric (int64)\")\n",
    "else:\n",
    "    print(\"✓ txId has correct data type\")\n",
    "\n",
    "# Check class values\n",
    "print(\"\\nClass value distribution:\")\n",
    "print(classes_df['class'].value_counts())\n",
    "\n",
    "valid_classes = {'1', '2', 'unknown'}\n",
    "invalid_classes = set(classes_df['class'].unique()) - valid_classes\n",
    "\n",
    "if len(invalid_classes) == 0:\n",
    "    print(\"\\n✓ All class values are valid ('1', '2', 'unknown')\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Invalid class values found: {invalid_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset 2: Transaction Edgelist (`elliptic_txs_edgelist.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (234355, 2)\n",
      "Columns: ['txId1', 'txId2']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId1</th>\n",
       "      <th>txId2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>5530458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232022460</td>\n",
       "      <td>232438397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230460314</td>\n",
       "      <td>230459870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230333930</td>\n",
       "      <td>230595899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232013274</td>\n",
       "      <td>232029206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       txId1      txId2\n",
       "0  230425980    5530458\n",
       "1  232022460  232438397\n",
       "2  230460314  230459870\n",
       "3  230333930  230595899\n",
       "4  232013274  232029206"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load transaction edgelist\n",
    "edges_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n",
    "\n",
    "print(f\"Shape: {edges_df.shape}\")\n",
    "print(f\"Columns: {list(edges_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      "       Missing_Count  Missing_Percentage\n",
      "txId1              0                 0.0\n",
      "txId2              0                 0.0\n",
      "\n",
      "✓ No missing values detected\n"
     ]
    }
   ],
   "source": [
    "# Count missing values\n",
    "missing_counts = edges_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(edges_df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_summary)\n",
    "\n",
    "if missing_summary['Missing_Count'].sum() == 0:\n",
    "    print(\"\\n✓ No missing values detected\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Total missing values: {missing_summary['Missing_Count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges: 234,355\n",
      "Duplicate edges: 0\n",
      "\n",
      "✓ No duplicate edges found\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate edges\n",
    "n_duplicates = edges_df.duplicated().sum()\n",
    "n_total = len(edges_df)\n",
    "\n",
    "print(f\"Total edges: {n_total:,}\")\n",
    "print(f\"Duplicate edges: {n_duplicates:,}\")\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(\"\\n✓ No duplicate edges found\")\n",
    "else:\n",
    "    print(f\"\\n⚠ {n_duplicates} duplicate edges detected\")\n",
    "    print(\"\\nDuplicate edge examples:\")\n",
    "    duplicated_edges = edges_df[edges_df.duplicated(keep=False)].sort_values(['txId1', 'txId2'])\n",
    "    print(duplicated_edges.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "txId1    int64\n",
      "txId2    int64\n",
      "dtype: object\n",
      "\n",
      "txId1 is numeric: True\n",
      "txId2 is numeric: True\n",
      "\n",
      "✓ Both columns have correct data types (numeric)\n",
      "\n",
      "Self-loops (txId1 == txId2): 0\n",
      "✓ No self-loops found\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"Data Types:\")\n",
    "print(edges_df.dtypes)\n",
    "\n",
    "# Validate both columns are numeric\n",
    "txId1_numeric = pd.api.types.is_numeric_dtype(edges_df['txId1'])\n",
    "txId2_numeric = pd.api.types.is_numeric_dtype(edges_df['txId2'])\n",
    "\n",
    "print(f\"\\ntxId1 is numeric: {txId1_numeric}\")\n",
    "print(f\"txId2 is numeric: {txId2_numeric}\")\n",
    "\n",
    "if txId1_numeric and txId2_numeric:\n",
    "    print(\"\\n✓ Both columns have correct data types (numeric)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Transaction IDs should be numeric (int64)\")\n",
    "\n",
    "# Check for self-loops (edges where txId1 == txId2)\n",
    "self_loops = (edges_df['txId1'] == edges_df['txId2']).sum()\n",
    "print(f\"\\nSelf-loops (txId1 == txId2): {self_loops:,}\")\n",
    "\n",
    "if self_loops == 0:\n",
    "    print(\"✓ No self-loops found\")\n",
    "else:\n",
    "    print(f\"⚠ {self_loops} self-loops detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset 3: Transaction Features (`elliptic_txs_features.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (203769, 167)\n",
      "\n",
      "First 3 rows (first 10 columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.12197</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.12197</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.12197</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1         2         3         4        5         6         7  \\\n",
       "0  230425980  1 -0.171469 -0.184668 -1.201369 -0.12197 -0.043875 -0.113002   \n",
       "1    5530458  1 -0.171484 -0.184668 -1.201369 -0.12197 -0.043875 -0.113002   \n",
       "2  232022460  1 -0.172107 -0.184668 -1.201369 -0.12197 -0.043875 -0.113002   \n",
       "\n",
       "          8         9  \n",
       "0 -0.061584 -0.162097  \n",
       "1 -0.061584 -0.162112  \n",
       "2 -0.061584 -0.162749  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load transaction features\n",
    "features_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_features.csv\", header=None)\n",
    "\n",
    "print(f\"Shape: {features_df.shape}\")\n",
    "print(f\"\\nFirst 3 rows (first 10 columns):\")\n",
    "features_df.iloc[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names assigned:\n",
      "Total columns: 167\n",
      "First 5 column names: ['txId', 'time_step', 'feature_1', 'feature_2', 'feature_3']\n",
      "Last 5 column names: ['feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165']\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for clarity\n",
    "# Column 0: txId\n",
    "# Column 1: time_step\n",
    "# Columns 2-167: feature_1 to feature_166\n",
    "feature_cols = ['txId', 'time_step'] + [f'feature_{i}' for i in range(1, 166)]\n",
    "features_df.columns = feature_cols\n",
    "\n",
    "print(\"Column names assigned:\")\n",
    "print(f\"Total columns: {len(features_df.columns)}\")\n",
    "print(f\"First 5 column names: {feature_cols[:5]}\")\n",
    "print(f\"Last 5 column names: {feature_cols[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 167\n",
      "Columns with missing values: 0\n",
      "\n",
      "✓ No missing values detected in any column\n"
     ]
    }
   ],
   "source": [
    "# Count missing values\n",
    "missing_counts = features_df.isnull().sum()\n",
    "columns_with_missing = missing_counts[missing_counts > 0]\n",
    "\n",
    "print(f\"Total columns: {len(features_df.columns)}\")\n",
    "print(f\"Columns with missing values: {len(columns_with_missing)}\")\n",
    "\n",
    "if len(columns_with_missing) == 0:\n",
    "    print(\"\\n✓ No missing values detected in any column\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Missing values found in {len(columns_with_missing)} columns\")\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Column': columns_with_missing.index,\n",
    "        'Missing_Count': columns_with_missing.values,\n",
    "        'Missing_Percentage': (columns_with_missing.values / len(features_df)) * 100\n",
    "    })\n",
    "    print(missing_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 203,769\n",
      "Unique transaction IDs: 203,769\n",
      "Duplicate transaction IDs: 0\n",
      "\n",
      "✓ No duplicate transaction IDs found\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate transaction IDs\n",
    "n_duplicates = features_df['txId'].duplicated().sum()\n",
    "n_unique = features_df['txId'].nunique()\n",
    "n_total = len(features_df)\n",
    "\n",
    "print(f\"Total records: {n_total:,}\")\n",
    "print(f\"Unique transaction IDs: {n_unique:,}\")\n",
    "print(f\"Duplicate transaction IDs: {n_duplicates:,}\")\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(\"\\n✓ No duplicate transaction IDs found\")\n",
    "else:\n",
    "    print(f\"\\n⚠ {n_duplicates} duplicate transaction IDs detected\")\n",
    "    print(\"\\nDuplicate entries (first 5):\")\n",
    "    duplicated_txs = features_df[features_df['txId'].duplicated(keep=False)].sort_values('txId')\n",
    "    print(duplicated_txs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types (first 10 columns):\n",
      "txId           int64\n",
      "time_step      int64\n",
      "feature_1    float64\n",
      "feature_2    float64\n",
      "feature_3    float64\n",
      "feature_4    float64\n",
      "feature_5    float64\n",
      "feature_6    float64\n",
      "feature_7    float64\n",
      "feature_8    float64\n",
      "dtype: object\n",
      "\n",
      "txId is numeric: True\n",
      "time_step is numeric: True\n",
      "\n",
      "Feature columns: 165\n",
      "Numeric feature columns: 165\n",
      "Non-numeric feature columns: 0\n",
      "\n",
      "✓ All feature columns are numeric\n"
     ]
    }
   ],
   "source": [
    "# Check data types for key columns\n",
    "print(\"Data Types (first 10 columns):\")\n",
    "print(features_df.dtypes[:10])\n",
    "\n",
    "# Validate txId is numeric\n",
    "txId_numeric = pd.api.types.is_numeric_dtype(features_df['txId'])\n",
    "print(f\"\\ntxId is numeric: {txId_numeric}\")\n",
    "\n",
    "# Validate time_step is numeric\n",
    "time_numeric = pd.api.types.is_numeric_dtype(features_df['time_step'])\n",
    "print(f\"time_step is numeric: {time_numeric}\")\n",
    "\n",
    "# Count numeric vs non-numeric feature columns\n",
    "feature_columns = [col for col in features_df.columns if col.startswith('feature_')]\n",
    "numeric_features = sum([pd.api.types.is_numeric_dtype(features_df[col]) for col in feature_columns])\n",
    "\n",
    "print(f\"\\nFeature columns: {len(feature_columns)}\")\n",
    "print(f\"Numeric feature columns: {numeric_features}\")\n",
    "print(f\"Non-numeric feature columns: {len(feature_columns) - numeric_features}\")\n",
    "\n",
    "if numeric_features == len(feature_columns):\n",
    "    print(\"\\n✓ All feature columns are numeric\")\n",
    "else:\n",
    "    print(f\"\\n⚠ {len(feature_columns) - numeric_features} feature columns are not numeric\")\n",
    "    non_numeric = [col for col in feature_columns if not pd.api.types.is_numeric_dtype(features_df[col])]\n",
    "    print(f\"Non-numeric columns: {non_numeric[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Step Summary:\n",
      "  Min: 1\n",
      "  Max: 49\n",
      "  Unique time steps: 49\n",
      "  Expected time steps: 49 (1-49)\n",
      "\n",
      "✓ Time step range is valid (1-49)\n"
     ]
    }
   ],
   "source": [
    "# Check time_step range\n",
    "print(\"Time Step Summary:\")\n",
    "print(f\"  Min: {features_df['time_step'].min()}\")\n",
    "print(f\"  Max: {features_df['time_step'].max()}\")\n",
    "print(f\"  Unique time steps: {features_df['time_step'].nunique()}\")\n",
    "print(f\"  Expected time steps: 49 (1-49)\")\n",
    "\n",
    "if features_df['time_step'].min() == 1 and features_df['time_step'].max() == 49:\n",
    "    print(\"\\n✓ Time step range is valid (1-49)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Time step range differs from expected (1-49)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cross-Dataset Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Transaction ID Consistency Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction ID counts by dataset:\n",
      "  Classes:  203,769 unique transaction IDs\n",
      "  Features: 203,769 unique transaction IDs\n",
      "  Edges:    203,769 unique transaction IDs (combined from txId1 and txId2)\n"
     ]
    }
   ],
   "source": [
    "# Get unique transaction IDs from each dataset\n",
    "txIds_classes = set(classes_df['txId'].unique())\n",
    "txIds_features = set(features_df['txId'].unique())\n",
    "txIds_edges_all = set(edges_df['txId1'].unique()) | set(edges_df['txId2'].unique())\n",
    "\n",
    "print(\"Transaction ID counts by dataset:\")\n",
    "print(f\"  Classes:  {len(txIds_classes):,} unique transaction IDs\")\n",
    "print(f\"  Features: {len(txIds_features):,} unique transaction IDs\")\n",
    "print(f\"  Edges:    {len(txIds_edges_all):,} unique transaction IDs (combined from txId1 and txId2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes vs Features:\n",
      "  Transaction IDs in classes but not in features: 0\n",
      "  Transaction IDs in features but not in classes: 0\n",
      "  ✓ Classes and features have identical transaction IDs\n"
     ]
    }
   ],
   "source": [
    "# Check if classes and features have the same transaction IDs\n",
    "classes_not_in_features = txIds_classes - txIds_features\n",
    "features_not_in_classes = txIds_features - txIds_classes\n",
    "\n",
    "print(\"\\nClasses vs Features:\")\n",
    "print(f\"  Transaction IDs in classes but not in features: {len(classes_not_in_features):,}\")\n",
    "print(f\"  Transaction IDs in features but not in classes: {len(features_not_in_classes):,}\")\n",
    "\n",
    "if len(classes_not_in_features) == 0 and len(features_not_in_classes) == 0:\n",
    "    print(\"  ✓ Classes and features have identical transaction IDs\")\n",
    "else:\n",
    "    print(\"  ⚠ Transaction ID mismatch between classes and features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edges vs Features:\n",
      "  Transaction IDs in edges but not in features: 0\n",
      "  ✓ All edge transaction IDs exist in features dataset\n"
     ]
    }
   ],
   "source": [
    "# Check if edge transaction IDs exist in features\n",
    "edges_not_in_features = txIds_edges_all - txIds_features\n",
    "\n",
    "print(\"\\nEdges vs Features:\")\n",
    "print(f\"  Transaction IDs in edges but not in features: {len(edges_not_in_features):,}\")\n",
    "\n",
    "if len(edges_not_in_features) == 0:\n",
    "    print(\"  ✓ All edge transaction IDs exist in features dataset\")\n",
    "else:\n",
    "    print(f\"  ⚠ {len(edges_not_in_features)} transaction IDs from edges are missing in features\")\n",
    "    print(f\"  Missing IDs (first 10): {list(edges_not_in_features)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY CHECK - SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. DATASET DIMENSIONS:\n",
      "   Classes:  203,769 rows × 2 columns\n",
      "   Edges:    234,355 rows × 2 columns\n",
      "   Features: 203,769 rows × 167 columns\n",
      "\n",
      "2. MISSING VALUES:\n",
      "   Classes:  0 missing values\n",
      "   Edges:    0 missing values\n",
      "   Features: 0 missing values\n",
      "   ✓ No missing values in any dataset\n",
      "\n",
      "3. DUPLICATES:\n",
      "   Classes:  0 duplicate transaction IDs\n",
      "   Edges:    0 duplicate edges\n",
      "   Features: 0 duplicate transaction IDs\n",
      "   ✓ No duplicates in any dataset\n",
      "\n",
      "4. DATA TYPES:\n",
      "   Classes:  ✓ txId is numeric\n",
      "   Edges:    ✓ txId1 and txId2 are numeric\n",
      "   Features: ✓ txId and time_step are numeric\n",
      "\n",
      "5. CROSS-DATASET CONSISTENCY:\n",
      "   ✓ Transaction IDs are consistent across all datasets\n",
      "\n",
      "======================================================================\n",
      "\n",
      "✓ ALL DATA QUALITY CHECKS PASSED\n",
      "  The datasets are clean and ready for analysis.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY CHECK - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATASET DIMENSIONS:\")\n",
    "print(f\"   Classes:  {classes_df.shape[0]:,} rows × {classes_df.shape[1]} columns\")\n",
    "print(f\"   Edges:    {edges_df.shape[0]:,} rows × {edges_df.shape[1]} columns\")\n",
    "print(f\"   Features: {features_df.shape[0]:,} rows × {features_df.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n2. MISSING VALUES:\")\n",
    "classes_missing = classes_df.isnull().sum().sum()\n",
    "edges_missing = edges_df.isnull().sum().sum()\n",
    "features_missing = features_df.isnull().sum().sum()\n",
    "\n",
    "print(f\"   Classes:  {classes_missing:,} missing values\")\n",
    "print(f\"   Edges:    {edges_missing:,} missing values\")\n",
    "print(f\"   Features: {features_missing:,} missing values\")\n",
    "\n",
    "if classes_missing + edges_missing + features_missing == 0:\n",
    "    print(\"   ✓ No missing values in any dataset\")\n",
    "else:\n",
    "    print(\"   ⚠ Missing values detected\")\n",
    "\n",
    "print(\"\\n3. DUPLICATES:\")\n",
    "classes_dupes = classes_df['txId'].duplicated().sum()\n",
    "edges_dupes = edges_df.duplicated().sum()\n",
    "features_dupes = features_df['txId'].duplicated().sum()\n",
    "\n",
    "print(f\"   Classes:  {classes_dupes:,} duplicate transaction IDs\")\n",
    "print(f\"   Edges:    {edges_dupes:,} duplicate edges\")\n",
    "print(f\"   Features: {features_dupes:,} duplicate transaction IDs\")\n",
    "\n",
    "if classes_dupes + edges_dupes + features_dupes == 0:\n",
    "    print(\"   ✓ No duplicates in any dataset\")\n",
    "else:\n",
    "    print(\"   ⚠ Duplicates detected\")\n",
    "\n",
    "print(\"\\n4. DATA TYPES:\")\n",
    "classes_types_ok = pd.api.types.is_numeric_dtype(classes_df['txId'])\n",
    "edges_types_ok = (pd.api.types.is_numeric_dtype(edges_df['txId1']) and \n",
    "                  pd.api.types.is_numeric_dtype(edges_df['txId2']))\n",
    "features_types_ok = (pd.api.types.is_numeric_dtype(features_df['txId']) and\n",
    "                     pd.api.types.is_numeric_dtype(features_df['time_step']))\n",
    "\n",
    "print(f\"   Classes:  {'✓' if classes_types_ok else '⚠'} txId is numeric\")\n",
    "print(f\"   Edges:    {'✓' if edges_types_ok else '⚠'} txId1 and txId2 are numeric\")\n",
    "print(f\"   Features: {'✓' if features_types_ok else '⚠'} txId and time_step are numeric\")\n",
    "\n",
    "print(\"\\n5. CROSS-DATASET CONSISTENCY:\")\n",
    "consistency_ok = (len(classes_not_in_features) == 0 and \n",
    "                  len(features_not_in_classes) == 0 and\n",
    "                  len(edges_not_in_features) == 0)\n",
    "\n",
    "if consistency_ok:\n",
    "    print(\"   ✓ Transaction IDs are consistent across all datasets\")\n",
    "else:\n",
    "    print(\"   ⚠ Transaction ID mismatches detected between datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Overall assessment\n",
    "all_checks_passed = (classes_missing + edges_missing + features_missing == 0 and\n",
    "                     classes_dupes + edges_dupes + features_dupes == 0 and\n",
    "                     classes_types_ok and edges_types_ok and features_types_ok and\n",
    "                     consistency_ok)\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(\"\\n✓ ALL DATA QUALITY CHECKS PASSED\")\n",
    "    print(\"  The datasets are clean and ready for analysis.\")\n",
    "else:\n",
    "    print(\"\\n⚠ SOME DATA QUALITY ISSUES DETECTED\")\n",
    "    print(\"  Review the warnings above for details.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
