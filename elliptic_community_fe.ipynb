{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community-Based Feature Engineering: Elliptic Bitcoin Dataset\n",
    "\n",
    "**Objective:** Engineer community-level features for all 203,769 transactions to improve GCN fraud detection performance\n",
    "\n",
    "**Context:**\n",
    "- Per-timestep community detection revealed 45 fraud rings with distinct structural signatures\n",
    "- Fraud communities have +38% higher density, -33% smaller size, and -75% lower clustering\n",
    "- These structural patterns can be used as features to improve model performance\n",
    "\n",
    "**Approach:**\n",
    "1. Run Louvain community detection on each of 49 timesteps\n",
    "2. Calculate 12 community-based features for every node\n",
    "3. Append features to original feature set (166 original â†’ 178 total)\n",
    "\n",
    "**New Features (indices 167-178):**\n",
    "- `community_density_167` - Density of node's community\n",
    "- `community_size_168` - Size of node's community\n",
    "- `community_clustering_169` - Average clustering of community\n",
    "- `community_unknown_ratio_170` - % unknown nodes in community\n",
    "- `is_small_dense_community_171` - Binary: density > 0.016 & size < 90\n",
    "- `is_zero_clustering_172` - Binary: clustering == 0.0\n",
    "- `community_illicit_pct_173` - % illicit nodes (among labeled)\n",
    "- `community_purity_174` - Max(illicit, licit) / total_labeled\n",
    "- `node_betweenness_centrality_175` - Node's betweenness in community\n",
    "- `node_degree_centrality_176` - Node's degree centrality in community\n",
    "- `community_avg_path_length_177` - Avg shortest path in community\n",
    "- `community_diameter_178` - Maximum shortest path in community\n",
    "\n",
    "**Expected Improvement:** +10-15% F1 score on temporal split (27.75% â†’ 38-43%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Community detection\n",
    "import community.community_louvain as community_louvain\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "print(\"Loading Elliptic Bitcoin Dataset...\\n\")\n",
    "\n",
    "# Load features (no header in original file)\n",
    "features_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_features.csv\", header=None)\n",
    "features_df.columns = ['txId', 'timestep'] + [f'feature_{i}' for i in range(1, 166)]\n",
    "\n",
    "# Load labels\n",
    "classes_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n",
    "\n",
    "# Load edges\n",
    "edgelist_df = pd.read_csv(\"raw_data/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n",
    "\n",
    "print(f\"Features: {features_df.shape}\")\n",
    "print(f\"Classes: {classes_df.shape}\")\n",
    "print(f\"Edges: {edgelist_df.shape}\")\n",
    "print(f\"\\nTime steps: {features_df['timestep'].nunique()}\")\n",
    "print(f\"Total nodes: {len(features_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mapping for fast lookup\n",
    "node_to_label = dict(zip(classes_df['txId'], classes_df['class']))\n",
    "\n",
    "# Display label distribution\n",
    "label_counts = classes_df['class'].value_counts()\n",
    "print(\"Label Distribution:\")\n",
    "print(f\"  Illicit (1): {label_counts.get('1', 0):,}\")\n",
    "print(f\"  Licit (2): {label_counts.get('2', 0):,}\")\n",
    "print(f\"  Unknown: {len(features_df) - len(classes_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Per-Timestep Community Detection\n",
    "\n",
    "Run Louvain algorithm on each timestep independently to respect temporal isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting per-timestep community detection...\")\n",
    "print(\"Estimated time: 2-5 minutes for 49 timesteps\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store community assignments and metrics for all nodes\n",
    "node_community_features = []\n",
    "\n",
    "# Process each timestep\n",
    "for ts in range(1, 50):\n",
    "    # Extract nodes for this timestep\n",
    "    ts_nodes = set(features_df[features_df['timestep'] == ts]['txId'].values)\n",
    "    \n",
    "    # Filter edges where BOTH nodes are in this timestep\n",
    "    edges_ts = edgelist_df[\n",
    "        (edgelist_df['txId1'].isin(ts_nodes)) & \n",
    "        (edgelist_df['txId2'].isin(ts_nodes))\n",
    "    ]\n",
    "    \n",
    "    # Build directed graph\n",
    "    G_ts = nx.from_pandas_edgelist(\n",
    "        edges_ts,\n",
    "        source='txId1',\n",
    "        target='txId2',\n",
    "        create_using=nx.DiGraph()\n",
    "    )\n",
    "    \n",
    "    # Add isolated nodes\n",
    "    for node in ts_nodes:\n",
    "        if node not in G_ts.nodes():\n",
    "            G_ts.add_node(node)\n",
    "    \n",
    "    # Convert to undirected for community detection\n",
    "    G_ts_undirected = G_ts.to_undirected()\n",
    "    \n",
    "    # Run Louvain community detection\n",
    "    communities = community_louvain.best_partition(G_ts_undirected)\n",
    "    \n",
    "    # Store for this timestep\n",
    "    node_community_features.append({\n",
    "        'timestep': ts,\n",
    "        'graph': G_ts,\n",
    "        'graph_undirected': G_ts_undirected,\n",
    "        'communities': communities,\n",
    "        'n_nodes': len(ts_nodes)\n",
    "    })\n",
    "    \n",
    "    # Progress update\n",
    "    if ts % 10 == 0:\n",
    "        print(f\"âœ“ Timestep {ts:2d}: {len(ts_nodes):5d} nodes, \"\n",
    "              f\"{len(set(communities.values())):3d} communities\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ“ Community detection complete for all 49 timesteps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Calculate Community-Level Metrics\n",
    "\n",
    "For each community, calculate structural metrics that will be assigned to all nodes in that community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating community-level structural metrics...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Process each timestep to calculate community metrics\n",
    "for ts_idx, ts_data in enumerate(node_community_features):\n",
    "    ts = ts_data['timestep']\n",
    "    G_ts = ts_data['graph']\n",
    "    G_undirected = ts_data['graph_undirected']\n",
    "    communities = ts_data['communities']\n",
    "    \n",
    "    # Group nodes by community\n",
    "    comm_to_nodes = defaultdict(list)\n",
    "    for node, comm_id in communities.items():\n",
    "        comm_to_nodes[comm_id].append(node)\n",
    "    \n",
    "    # Calculate metrics for each community\n",
    "    community_metrics = {}\n",
    "    \n",
    "    for comm_id, comm_nodes in comm_to_nodes.items():\n",
    "        # Create subgraph for this community\n",
    "        G_comm = G_undirected.subgraph(comm_nodes).copy()\n",
    "        \n",
    "        # Basic metrics\n",
    "        size = len(comm_nodes)\n",
    "        n_edges = G_comm.number_of_edges()\n",
    "        \n",
    "        # Calculate metrics (handle edge cases)\n",
    "        if size == 1:\n",
    "            # Isolated node - set all metrics to 0\n",
    "            density = 0.0\n",
    "            avg_clustering = 0.0\n",
    "            avg_path_length = 0.0\n",
    "            diameter = 0.0\n",
    "        else:\n",
    "            # Density\n",
    "            density = nx.density(G_comm)\n",
    "            \n",
    "            # Clustering\n",
    "            avg_clustering = nx.average_clustering(G_comm)\n",
    "            \n",
    "            # Path length and diameter (only for connected components)\n",
    "            if nx.is_connected(G_comm):\n",
    "                avg_path_length = nx.average_shortest_path_length(G_comm)\n",
    "                diameter = nx.diameter(G_comm)\n",
    "            else:\n",
    "                # Use largest connected component\n",
    "                largest_cc = max(nx.connected_components(G_comm), key=len)\n",
    "                G_cc = G_comm.subgraph(largest_cc).copy()\n",
    "                if len(G_cc) > 1:\n",
    "                    avg_path_length = nx.average_shortest_path_length(G_cc)\n",
    "                    diameter = nx.diameter(G_cc)\n",
    "                else:\n",
    "                    avg_path_length = 0.0\n",
    "                    diameter = 0.0\n",
    "        \n",
    "        # Label composition\n",
    "        illicit_count = 0\n",
    "        licit_count = 0\n",
    "        unknown_count = 0\n",
    "        \n",
    "        for node in comm_nodes:\n",
    "            label = node_to_label.get(node, 'unknown')\n",
    "            if label == '1':\n",
    "                illicit_count += 1\n",
    "            elif label == '2':\n",
    "                licit_count += 1\n",
    "            else:\n",
    "                unknown_count += 1\n",
    "        \n",
    "        # Calculate label-based metrics\n",
    "        total_labeled = illicit_count + licit_count\n",
    "        unknown_ratio = unknown_count / size if size > 0 else 0.0\n",
    "        illicit_pct = illicit_count / total_labeled if total_labeled > 0 else 0.0\n",
    "        purity = max(illicit_count, licit_count) / total_labeled if total_labeled > 0 else 0.0\n",
    "        \n",
    "        # Derived binary features\n",
    "        is_small_dense = 1 if (density > 0.016 and size < 90) else 0\n",
    "        is_zero_clustering = 1 if avg_clustering == 0.0 else 0\n",
    "        \n",
    "        # Store community-level metrics\n",
    "        community_metrics[comm_id] = {\n",
    "            'density': density,\n",
    "            'size': size,\n",
    "            'clustering': avg_clustering,\n",
    "            'unknown_ratio': unknown_ratio,\n",
    "            'is_small_dense': is_small_dense,\n",
    "            'is_zero_clustering': is_zero_clustering,\n",
    "            'illicit_pct': illicit_pct,\n",
    "            'purity': purity,\n",
    "            'avg_path_length': avg_path_length,\n",
    "            'diameter': diameter,\n",
    "            'subgraph': G_comm  # Store for node-level calculations\n",
    "        }\n",
    "    \n",
    "    # Store back\n",
    "    ts_data['community_metrics'] = community_metrics\n",
    "    \n",
    "    if ts % 10 == 0:\n",
    "        print(f\"âœ“ Timestep {ts:2d}: Calculated metrics for {len(community_metrics)} communities\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ“ Community-level metrics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Calculate Node-Level Centrality Metrics\n",
    "\n",
    "Calculate betweenness and degree centrality for each node within its community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating node-level centrality metrics within communities...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate centrality metrics for each node\n",
    "for ts_idx, ts_data in enumerate(node_community_features):\n",
    "    ts = ts_data['timestep']\n",
    "    community_metrics = ts_data['community_metrics']\n",
    "    \n",
    "    # Calculate centrality for each community\n",
    "    for comm_id, metrics in community_metrics.items():\n",
    "        G_comm = metrics['subgraph']\n",
    "        \n",
    "        if metrics['size'] == 1:\n",
    "            # Isolated node - centrality is 0\n",
    "            metrics['node_centralities'] = {\n",
    "                list(G_comm.nodes())[0]: {\n",
    "                    'betweenness': 0.0,\n",
    "                    'degree': 0.0\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # Calculate betweenness centrality\n",
    "            betweenness = nx.betweenness_centrality(G_comm)\n",
    "            \n",
    "            # Calculate degree centrality\n",
    "            degree_cent = nx.degree_centrality(G_comm)\n",
    "            \n",
    "            # Store per-node\n",
    "            node_centralities = {}\n",
    "            for node in G_comm.nodes():\n",
    "                node_centralities[node] = {\n",
    "                    'betweenness': betweenness.get(node, 0.0),\n",
    "                    'degree': degree_cent.get(node, 0.0)\n",
    "                }\n",
    "            \n",
    "            metrics['node_centralities'] = node_centralities\n",
    "    \n",
    "    if ts % 10 == 0:\n",
    "        print(f\"âœ“ Timestep {ts:2d}: Calculated node centralities\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ“ Node-level centrality metrics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Assign Features to All Nodes\n",
    "\n",
    "Create feature vectors for all 203,769 nodes with the 12 new community-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assigning community features to all nodes...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create list to store all node features\n",
    "all_node_features = []\n",
    "\n",
    "# Process each timestep\n",
    "for ts_data in node_community_features:\n",
    "    ts = ts_data['timestep']\n",
    "    communities = ts_data['communities']\n",
    "    community_metrics = ts_data['community_metrics']\n",
    "    \n",
    "    # Assign features to each node\n",
    "    for node, comm_id in communities.items():\n",
    "        # Get community-level metrics\n",
    "        comm_metrics = community_metrics[comm_id]\n",
    "        \n",
    "        # Get node-level centrality\n",
    "        node_cent = comm_metrics['node_centralities'].get(node, {'betweenness': 0.0, 'degree': 0.0})\n",
    "        \n",
    "        # Create feature vector (12 features: indices 167-178)\n",
    "        node_features = {\n",
    "            'txId': node,\n",
    "            'timestep': ts,\n",
    "            'community_density_167': comm_metrics['density'],\n",
    "            'community_size_168': comm_metrics['size'],\n",
    "            'community_clustering_169': comm_metrics['clustering'],\n",
    "            'community_unknown_ratio_170': comm_metrics['unknown_ratio'],\n",
    "            'is_small_dense_community_171': comm_metrics['is_small_dense'],\n",
    "            'is_zero_clustering_172': comm_metrics['is_zero_clustering'],\n",
    "            'community_illicit_pct_173': comm_metrics['illicit_pct'],\n",
    "            'community_purity_174': comm_metrics['purity'],\n",
    "            'node_betweenness_centrality_175': node_cent['betweenness'],\n",
    "            'node_degree_centrality_176': node_cent['degree'],\n",
    "            'community_avg_path_length_177': comm_metrics['avg_path_length'],\n",
    "            'community_diameter_178': comm_metrics['diameter']\n",
    "        }\n",
    "        \n",
    "        all_node_features.append(node_features)\n",
    "    \n",
    "    if ts % 10 == 0:\n",
    "        print(f\"âœ“ Timestep {ts:2d}: Assigned features to {len(communities)} nodes\")\n",
    "\n",
    "# Create DataFrame\n",
    "community_features_df = pd.DataFrame(all_node_features)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ“ Feature assignment complete!\")\n",
    "print(f\"  Total nodes: {len(community_features_df):,}\")\n",
    "print(f\"  Community features: {len(community_features_df.columns) - 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of new features\n",
    "print(\"\\nSample of new community features:\")\n",
    "print(community_features_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMMUNITY FEATURE SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(community_features_df.iloc[:, 2:].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Merge with Original Features\n",
    "\n",
    "Combine the 12 new community features with the original 165 features to create the final feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging community features with original features...\\n\")\n",
    "\n",
    "# Merge on txId and timestep\n",
    "features_with_community = features_df.merge(\n",
    "    community_features_df,\n",
    "    on=['txId', 'timestep'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Original features shape: {features_df.shape}\")\n",
    "print(f\"Community features shape: {community_features_df.shape}\")\n",
    "print(f\"Merged features shape: {features_with_community.shape}\")\n",
    "\n",
    "# Verify no missing values in community features (all nodes should have assignments)\n",
    "community_cols = [col for col in features_with_community.columns if '_1' in col and col.startswith('community') or col.startswith('node_') or col.startswith('is_')]\n",
    "missing = features_with_community[community_cols].isnull().sum().sum()\n",
    "\n",
    "if missing > 0:\n",
    "    print(f\"\\nâš  Warning: {missing} missing values in community features\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ No missing values - all nodes have community assignments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final feature set structure\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL FEATURE SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total columns: {len(features_with_community.columns)}\")\n",
    "print(f\"  - txId: 1 column\")\n",
    "print(f\"  - timestep: 1 column\")\n",
    "print(f\"  - Original features (1-165): 165 columns\")\n",
    "print(f\"  - Community features (167-178): 12 columns\")\n",
    "print(f\"\\nTotal features for modeling: {len(features_with_community.columns) - 2} (excluding txId and timestep)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"\\nColumn structure:\")\n",
    "print(f\"  First 5 columns: {list(features_with_community.columns[:5])}\")\n",
    "print(f\"  Last 12 columns (new features): {list(features_with_community.columns[-12:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Distribution Analysis\n",
    "\n",
    "Analyze the distribution of new community features by class (illicit vs. licit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Analyzing feature distributions by class...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to features DataFrame\n",
    "features_with_labels = features_with_community.copy()\n",
    "features_with_labels['label'] = features_with_labels['txId'].map(node_to_label)\n",
    "\n",
    "# Filter to labeled data only\n",
    "labeled_data = features_with_labels[features_with_labels['label'].isin(['1', '2'])].copy()\n",
    "labeled_data['label_name'] = labeled_data['label'].map({'1': 'Illicit', '2': 'Licit'})\n",
    "\n",
    "print(f\"Labeled data: {len(labeled_data):,} nodes\")\n",
    "print(f\"  Illicit: {(labeled_data['label'] == '1').sum():,}\")\n",
    "print(f\"  Licit: {(labeled_data['label'] == '2').sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare key features by class\n",
    "comparison_features = [\n",
    "    'community_density_167',\n",
    "    'community_size_168',\n",
    "    'community_clustering_169',\n",
    "    'community_unknown_ratio_170'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(comparison_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create violin plots\n",
    "    data_to_plot = [\n",
    "        labeled_data[labeled_data['label'] == '1'][feature].dropna(),\n",
    "        labeled_data[labeled_data['label'] == '2'][feature].dropna()\n",
    "    ]\n",
    "    \n",
    "    parts = ax.violinplot(data_to_plot, positions=[1, 2], showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Color the violins\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('#e74c3c')\n",
    "        pc.set_alpha(0.7)\n",
    "    parts['bodies'][1].set_facecolor('#2ecc71')\n",
    "    \n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(['Illicit', 'Licit'])\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(feature.replace('_', ' ').title(), fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Look for differences between illicit and licit distributions\")\n",
    "print(\"   These differences indicate discriminative power for fraud detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL COMPARISON: ILLICIT vs LICIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "community_feature_cols = [col for col in features_with_community.columns \n",
    "                          if col.startswith('community_') or col.startswith('node_') \n",
    "                          or col.startswith('is_')]\n",
    "\n",
    "illicit_data = labeled_data[labeled_data['label'] == '1']\n",
    "licit_data = labeled_data[labeled_data['label'] == '2']\n",
    "\n",
    "for feature in community_feature_cols:\n",
    "    illicit_vals = illicit_data[feature].dropna()\n",
    "    licit_vals = licit_data[feature].dropna()\n",
    "    \n",
    "    # Mann-Whitney U test\n",
    "    stat, p_value = mannwhitneyu(illicit_vals, licit_vals, alternative='two-sided')\n",
    "    \n",
    "    illicit_mean = illicit_vals.mean()\n",
    "    licit_mean = licit_vals.mean()\n",
    "    diff = illicit_mean - licit_mean\n",
    "    \n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    \n",
    "    print(f\"{feature:40s} | Illicit: {illicit_mean:8.4f} | Licit: {licit_mean:8.4f} | \"\n",
    "          f\"Diff: {diff:+8.4f} | p={p_value:.2e} {significance}\")\n",
    "\n",
    "print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Enhanced Feature Set\n",
    "\n",
    "Save the final feature set with community features to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"engineered_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Creating output directory: {output_dir}/\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"EXPORTING ENHANCED FEATURE SET\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for export (no headers, matching original format)\n",
    "# Original format: txId, timestep, feature_1, ..., feature_165 (no headers)\n",
    "# New format: txId, timestep, feature_1, ..., feature_165, community_density_167, ..., community_diameter_178\n",
    "\n",
    "output_path = f\"{output_dir}/elliptic_txs_features_with_community.csv\"\n",
    "\n",
    "# Save without headers to match original format\n",
    "features_with_community.to_csv(output_path, index=False, header=False)\n",
    "\n",
    "print(f\"âœ“ Enhanced features saved to: {output_path}\")\n",
    "print(f\"  Shape: {features_with_community.shape}\")\n",
    "print(f\"  Format: No headers (matches original format)\")\n",
    "print(f\"  Columns: txId, timestep, feature_1...feature_165, community_density_167...community_diameter_178\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save a version WITH headers for easier inspection\n",
    "output_path_with_headers = f\"{output_dir}/elliptic_txs_features_with_community_headers.csv\"\n",
    "features_with_community.to_csv(output_path_with_headers, index=False, header=True)\n",
    "\n",
    "print(f\"\\nâœ“ Version with headers saved to: {output_path_with_headers}\")\n",
    "print(f\"  (For inspection/debugging - use headerless version for modeling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature metadata for documentation\n",
    "feature_metadata = {\n",
    "    'Feature Index': list(range(167, 179)),\n",
    "    'Feature Name': [\n",
    "        'community_density_167',\n",
    "        'community_size_168',\n",
    "        'community_clustering_169',\n",
    "        'community_unknown_ratio_170',\n",
    "        'is_small_dense_community_171',\n",
    "        'is_zero_clustering_172',\n",
    "        'community_illicit_pct_173',\n",
    "        'community_purity_174',\n",
    "        'node_betweenness_centrality_175',\n",
    "        'node_degree_centrality_176',\n",
    "        'community_avg_path_length_177',\n",
    "        'community_diameter_178'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Density of the community (edges / possible edges)',\n",
    "        'Number of nodes in the community',\n",
    "        'Average clustering coefficient of the community',\n",
    "        'Ratio of unknown nodes in the community',\n",
    "        'Binary flag: density > 0.016 AND size < 90',\n",
    "        'Binary flag: clustering coefficient == 0.0',\n",
    "        'Percentage of illicit nodes (among labeled nodes)',\n",
    "        'Purity: max(illicit, licit) / total_labeled',\n",
    "        'Node betweenness centrality within its community',\n",
    "        'Node degree centrality within its community',\n",
    "        'Average shortest path length in the community',\n",
    "        'Diameter (longest shortest path) of the community'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame(feature_metadata)\n",
    "metadata_path = f\"{output_dir}/community_features_metadata.csv\"\n",
    "metadata_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Feature metadata saved to: {metadata_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMMUNITY-BASED FEATURE ENGINEERING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATA PROCESSING\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   â€¢ Total nodes processed: {len(features_with_community):,}\")\n",
    "print(f\"   â€¢ Time steps processed: 49\")\n",
    "print(f\"   â€¢ Communities detected: {len(community_features_df):,} node-community assignments\")\n",
    "\n",
    "print(\"\\n2. NEW FEATURES CREATED\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   â€¢ Community-level features: 8\")\n",
    "print(f\"     - Structural: density, size, clustering, avg_path_length, diameter\")\n",
    "print(f\"     - Compositional: unknown_ratio, illicit_pct, purity\")\n",
    "print(f\"   â€¢ Derived binary features: 2\")\n",
    "print(f\"     - is_small_dense_community, is_zero_clustering\")\n",
    "print(f\"   â€¢ Node-level features: 2\")\n",
    "print(f\"     - node_betweenness_centrality, node_degree_centrality\")\n",
    "print(f\"   â€¢ Total new features: 12 (indices 167-178)\")\n",
    "\n",
    "print(\"\\n3. FINAL FEATURE SET\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   â€¢ Original features: 165 (feature_1 to feature_165)\")\n",
    "print(f\"   â€¢ Community features: 12 (community_density_167 to community_diameter_178)\")\n",
    "print(f\"   â€¢ Total features: 177\")\n",
    "print(f\"   â€¢ Feature types:\")\n",
    "print(f\"     - Local (1-94): 94 features\")\n",
    "print(f\"     - Aggregated (95-165): 71 features\")\n",
    "print(f\"     - Community-based (167-178): 12 features\")\n",
    "\n",
    "print(\"\\n4. KEY FINDINGS FROM DISTRIBUTION ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "illicit_density = labeled_data[labeled_data['label'] == '1']['community_density_167'].mean()\n",
    "licit_density = labeled_data[labeled_data['label'] == '2']['community_density_167'].mean()\n",
    "density_diff_pct = (illicit_density - licit_density) / licit_density * 100\n",
    "\n",
    "illicit_size = labeled_data[labeled_data['label'] == '1']['community_size_168'].mean()\n",
    "licit_size = labeled_data[labeled_data['label'] == '2']['community_size_168'].mean()\n",
    "size_diff_pct = (illicit_size - licit_size) / licit_size * 100\n",
    "\n",
    "print(f\"   â€¢ Illicit nodes have {density_diff_pct:+.1f}% different community density\")\n",
    "print(f\"   â€¢ Illicit nodes have {size_diff_pct:+.1f}% different community size\")\n",
    "print(f\"   â€¢ These structural differences validate fraud ring signatures\")\n",
    "\n",
    "print(\"\\n5. OUTPUT FILES\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   â€¢ {output_dir}/elliptic_txs_features_with_community.csv\")\n",
    "print(f\"     (Main file - no headers, 203,769 rows Ã— 179 columns)\")\n",
    "print(f\"   â€¢ {output_dir}/elliptic_txs_features_with_community_headers.csv\")\n",
    "print(f\"     (With headers for inspection)\")\n",
    "print(f\"   â€¢ {output_dir}/community_features_metadata.csv\")\n",
    "print(f\"     (Feature documentation)\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS\")\n",
    "print(\"-\" * 70)\n",
    "print(\"   âœ“ Use enhanced features in GCN model training\")\n",
    "print(\"   âœ“ Expected improvement: +10-15% F1 on temporal split\")\n",
    "print(\"   âœ“ Test different feature combinations:\")\n",
    "print(\"     - Original only (165 features)\")\n",
    "print(\"     - Original + Community (177 features)\")\n",
    "print(\"     - Community only (12 features) - baseline\")\n",
    "print(\"   âœ“ Analyze feature importance to identify most predictive features\")\n",
    "print(\"   âœ“ Consider feature selection to reduce dimensionality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ COMMUNITY-BASED FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
